services:
  # DeepSeek OCR Service (vLLM)
  vllm:
    build:
      context: ./deepseek-ocr-service
      dockerfile: Dockerfile.vllm
    ports:
      - "8000:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    volumes:
      - ./.cache/huggingface:/root/.cache/huggingface
    # Added --gpu-memory-utilization 0.45
    command: vllm serve deepseek-ai/DeepSeek-OCR --logits_processors vllm.model_executor.models.deepseek_ocr:NGramPerReqLogitsProcessor --no-enable-prefix-caching --mm-processor-cache-gb 0 --port 8000 --trust-remote-code --gpu-memory-utilization 0.45
    restart: unless-stopped

  # DeepSeek OCR Backend (FastAPI)
  backend:
    build:
      context: ./deepseek-ocr-service
      dockerfile: Dockerfile
    ports:
      - "8001:8001"
    environment:
      - VLLM_URL=http://vllm:8000/v1
    depends_on:
      - vllm
    restart: unless-stopped

  # Qwen LLM Service (vLLM)
  qwen-vllm:
    build:
      context: ./qwen-llm-service
      dockerfile: Dockerfile.vllm
    ports:
      - "8002:8002"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    volumes:
      - ./.cache/huggingface:/root/.cache/huggingface
    command: vllm serve RedHatAI/Qwen3-8B-FP8-dynamic --gpu-memory-utilization 0.45 --port 8002 --trust-remote-code --dtype float16
    restart: unless-stopped

  # Qwen LLM Backend (FastAPI)
  qwen-backend:
    build:
      context: ./qwen-llm-service
      dockerfile: Dockerfile
    ports:
      - "8003:8003"
    environment:
      - VLLM_URL=http://qwen-vllm:8002/v1
    depends_on:
      - qwen-vllm
    restart: unless-stopped

  # Frontend (React + Nginx)
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "80:80"
    depends_on:
      - backend
      - qwen-backend
    restart: unless-stopped
