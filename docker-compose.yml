services:
  # DeepSeek OCR Service (vLLM)
  vllm:
    build:
      context: ./deepseek-ocr-service
      dockerfile: Dockerfile.vllm
    ports:
      - "8000:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    volumes:
      - ./.cache/huggingface:/root/.cache/huggingface
    # Increased to 0.60 for better performance
    command: vllm serve deepseek-ai/DeepSeek-OCR --logits_processors vllm.model_executor.models.deepseek_ocr:NGramPerReqLogitsProcessor --no-enable-prefix-caching --mm-processor-cache-gb 0 --port 8000 --trust-remote-code --gpu-memory-utilization 0.60
    restart: unless-stopped

  # DeepSeek OCR Backend (FastAPI)
  backend:
    build:
      context: ./deepseek-ocr-service
      dockerfile: Dockerfile
    ports:
      - "8001:8001"
    environment:
      - VLLM_URL=http://vllm:8000/v1
    depends_on:
      - vllm
    restart: unless-stopped

  # Local LLM Service (Turkish-Gemma-9B, currently disabled)
  local-llm-service:
    build:
      context: ./local-llm-service
      dockerfile: Dockerfile
    ports:
      - "8003:8003"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    volumes:
      - ./.cache/huggingface:/root/.cache/huggingface
    restart: unless-stopped
    profiles:
      - disabled  # This service is disabled by default

  # Frontend (React + Nginx)
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "80:80"
    depends_on:
      - backend
    restart: unless-stopped
