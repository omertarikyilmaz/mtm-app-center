services:
  # =====================================================
  # DISABLED SERVICES (profiles: disabled)
  # =====================================================
  
  # DeepSeek OCR - Model Server (vLLM)
  deepseek-ocr-vllm:
    build:
      context: ./deepseek-ocr-service
      dockerfile: Dockerfile.vllm
    ports:
      - "8101:8101"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    volumes:
      - ./.cache/huggingface:/root/.cache/huggingface
    command: vllm serve deepseek-ai/DeepSeek-OCR --logits_processors vllm.model_executor.models.deepseek_ocr:NGramPerReqLogitsProcessor --no-enable-prefix-caching --mm-processor-cache-gb 0 --port 8101 --trust-remote-code --gpu-memory-utilization 0.45
    restart: unless-stopped
    profiles:
      - disabled

  # DeepSeek OCR - API Service (FastAPI)
  deepseek-ocr-api:
    build:
      context: ./deepseek-ocr-service
      dockerfile: Dockerfile
    ports:
      - "8001:8001"
    environment:
      - VLLM_URL=http://deepseek-ocr-vllm:8101/v1
    depends_on:
      - deepseek-ocr-vllm
    restart: unless-stopped
    profiles:
      - disabled

  # İflas OCR Pipeline - API Service (OpenAI + DeepSeek OCR)
  iflas-pipeline-api:
    build:
      context: ./pipelines/openai-iflas-pipeline
      dockerfile: Dockerfile
    ports:
      - "8003:8003"
    environment:
      - DEEPSEEK_OCR_URL=http://deepseek-ocr-api:8001/api/v1/ocr
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
    depends_on:
      - deepseek-ocr-api
    restart: unless-stopped
    profiles:
      - disabled

  # MBR Künye Pipeline - API Service (OpenAI + DeepSeek OCR)
  mbr-kunye-pipeline:
    build:
      context: ./pipelines/mbr-kunye-pipeline
      dockerfile: Dockerfile
    ports:
      - "8006:8006"
    environment:
      - DEEPSEEK_OCR_URL=http://deepseek-ocr-api:8001/api/v1/ocr
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
    depends_on:
      - deepseek-ocr-api
    restart: unless-stopped
    profiles:
      - disabled

  # MBR Künye Web Pipeline - API Service (OpenAI + Web Scraping)
  mbr-kunye-web-pipeline:
    build:
      context: ./pipelines/mbr-kunye-web-pipeline
      dockerfile: Dockerfile
    ports:
      - "8007:8007"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
    restart: unless-stopped
    profiles:
      - disabled

  # Local Turkish-Gemma LLM - API Service
  local-llm-api:
    build:
      context: ./local-llm-service
      dockerfile: Dockerfile
    ports:
      - "8004:8004"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    volumes:
      - ./.cache/huggingface:/root/.cache/huggingface
    restart: unless-stopped
    profiles:
      - disabled

  # Radyo News Pipeline - API Service (OpenAI Whisper + GPT)
  radyo-news-pipeline:
    build:
      context: ./pipelines/openai-radyo-pipeline
      dockerfile: Dockerfile
    ports:
      - "8008:8008"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
    volumes:
      - /tmp/audio:/tmp/audio
    restart: unless-stopped
    profiles:
      - disabled

  # AI Data Analyst Pipeline - API Service (OpenAI GPT + Web Scraping)
  ai-data-analyst:
    build:
      context: ./pipelines/ai-data-analyst
      dockerfile: Dockerfile
    ports:
      - "8009:8009"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
    restart: unless-stopped
    profiles:
      - disabled

  # =====================================================
  # ACTIVE SERVICES (SAM-Audio Focus Mode)
  # =====================================================

  # Frontend - Web UI (React + Nginx)
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "80:80"
    restart: unless-stopped

  # SAM-Audio Pipeline - Audio Source Separation (Meta SAM-Audio)
  sam-audio-pipeline:
    build:
      context: ./pipelines/sam-audio-pipeline
      dockerfile: Dockerfile
    ports:
      - "8011:8011"
    environment:
      - HF_TOKEN=${HF_TOKEN:-}
      - SAM_MODEL=facebook/sam-audio-small
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    volumes:
      - ./.cache/huggingface:/root/.cache/huggingface
      - /tmp/sam-audio:/tmp/sam-audio
    restart: unless-stopped
