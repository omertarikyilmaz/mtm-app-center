FROM pytorch/pytorch:2.4.0-cuda12.1-cudnn9-devel

# Set working directory
WORKDIR /workspace

# Install system dependencies
RUN apt-get update && apt-get install -y \
    git \
    wget \
    && rm -rf /var/lib/apt/lists/*

# Install vLLM and dependencies
RUN pip install --no-cache-dir vllm transformers pillow accelerate

# Pre-download the model (optional, but recommended)
# This will cache the model in the Docker image
RUN python -c "from transformers import AutoTokenizer, AutoProcessor; \
    AutoTokenizer.from_pretrained('tencent/HunyuanOCR', trust_remote_code=True); \
    AutoProcessor.from_pretrained('tencent/HunyuanOCR', trust_remote_code=True)"

# The actual model weights will be downloaded at runtime and cached in the volume
# This is just to prepare the environment

# Expose vLLM port
EXPOSE 8005

# Start vLLM server
# Note: The command will be overridden in docker-compose.yml
CMD ["python", "-m", "vllm.entrypoints.openai.api_server", \
     "--model", "tencent/HunyuanOCR", \
     "--host", "0.0.0.0", \
     "--port", "8005", \
     "--trust-remote-code", \
     "--gpu-memory-utilization", "0.45"]
