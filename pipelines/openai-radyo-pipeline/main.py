from fastapi import FastAPI, HTTPException, UploadFile, File, Form, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel
from typing import List, Optional
import openai
import os
import json
import tempfile
import asyncio
from pathlib import Path

app = FastAPI(title="MTM Radyo News Pipeline", version="2.0.0")

# Configuration
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', '')
TEMP_AUDIO_DIR = Path("/tmp/audio")
TEMP_AUDIO_DIR.mkdir(exist_ok=True, parents=True)

# Models
class NewsItem(BaseModel):
    """Structured news item"""
    baslik: str
    kategori: str  # politika, ekonomi, spor, saglik, teknoloji, guncel, diger
    ozet: str  # 2-3 cÃ¼mle
    tam_metin: str
    tarih: Optional[str] = None
    kisiler: Optional[List[str]] = None
    kurumlar: Optional[List[str]] = None
    yerler: Optional[List[str]] = None
    ozel_isimler: Optional[List[str]] = None  # TÃ¼m Ã¶zel isimler (proper nouns)

class RadioAnalysisResult(BaseModel):
    """Complete analysis result"""
    total_news_count: int
    categories: dict  # kategori: sayÄ±
    news_items: List[NewsItem]
    raw_transcript: str

@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    import traceback
    error_details = traceback.format_exc()
    print(f"Global Error: {error_details}")
    return JSONResponse(
        status_code=500,
        content={"detail": f"Server Error: {str(exc)}"},
    )

@app.get("/")
async def root():
    return {"status": "running", "service": "openai-radyo-pipeline", "version": "2.0.0"}

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Core Functions

async def transcribe_audio(audio_path: Path, api_key: str) -> str:
    """
    Transcribe entire audio file using OpenAI Whisper API.
    For large files (>25MB), automatically chunks into segments.
    
    Args:
        audio_path: Path to audio file
        api_key: OpenAI API key
    
    Returns:
        Full transcript text
    """
    try:
        from pydub import AudioSegment
        import math
        
        print(f"[DEBUG] Transcribing audio: {audio_path}")
        
        # Load audio file
        audio = AudioSegment.from_file(str(audio_path))
        duration_ms = len(audio)
        duration_min = duration_ms / 60000
        
        print(f"[DEBUG] Audio duration: {duration_min:.1f} minutes")
        
        # Chunk size: 10 minutes (600,000 ms)
        CHUNK_DURATION_MS = 10 * 60 * 1000
        
        # If file is small enough, process directly
        if duration_ms <= CHUNK_DURATION_MS:
            print(f"[DEBUG] File small enough, direct transcription")
            client = openai.OpenAI(api_key=api_key)
            
            with open(audio_path, 'rb') as audio_file:
                transcript = client.audio.transcriptions.create(
                    model="whisper-1",
                    file=audio_file,
                    language="tr",
                    response_format="text"
                )
            
            print(f"[DEBUG] Transcription complete ({len(transcript)} chars)")
            return transcript
        
        # Large file - chunk it
        num_chunks = math.ceil(duration_ms / CHUNK_DURATION_MS)
        print(f"[DEBUG] Large file, splitting into {num_chunks} chunks")
        
        transcripts = []
        client = openai.OpenAI(api_key=api_key)
        
        for i in range(num_chunks):
            start_ms = i * CHUNK_DURATION_MS
            end_ms = min((i + 1) * CHUNK_DURATION_MS, duration_ms)
            
            print(f"[DEBUG] Processing chunk {i+1}/{num_chunks} ({start_ms/60000:.1f}-{end_ms/60000:.1f} min)")
            
            # Extract chunk
            chunk = audio[start_ms:end_ms]
            
            # Save chunk to temp file
            chunk_path = audio_path.parent / f"chunk_{i}_{audio_path.name}"
            chunk.export(str(chunk_path), format="mp3")
            
            try:
                # Transcribe chunk
                with open(chunk_path, 'rb') as chunk_file:
                    chunk_transcript = client.audio.transcriptions.create(
                        model="whisper-1",
                        file=chunk_file,
                        language="tr",
                        response_format="text"
                    )
                
                transcripts.append(chunk_transcript)
                print(f"[DEBUG] Chunk {i+1} transcribed ({len(chunk_transcript)} chars)")
                
            finally:
                # Clean up chunk file
                if chunk_path.exists():
                    chunk_path.unlink()
        
        # Combine all transcripts
        full_transcript = " ".join(transcripts)
        print(f"[DEBUG] All chunks transcribed, total: {len(full_transcript)} chars")
        
        return full_transcript
        
    except Exception as e:
        print(f"[ERROR] Transcription failed: {e}")
        raise

def create_news_extraction_prompt() -> str:
    """Create detailed prompt for GPT to extract news from transcript"""
    return """Sen bir profesyonel radyo haber analisti ve editÃ¶rÃ¼sÃ¼n. Uzun yÄ±llardÄ±r radyo yayÄ±nlarÄ±nÄ± analiz ediyorsun.

**GÃ–REVÄ°N:**
AÅŸaÄŸÄ±da bir radyo kanalÄ±nÄ±n 1 saatlik yayÄ±n transkripsiyonu verilecek. Bu transkriptte:
- Haberler
- Reklamlar
- MÃ¼zik programlarÄ±
- Jingle'lar
- Sunucu sohbetleri
- Program tanÄ±tÄ±mlarÄ±
karÄ±ÅŸÄ±k halde bulunuyor.

**SADECE HABER Ä°Ã‡ERÄ°KLERÄ°NÄ°** tespit edip yapÄ±landÄ±rÄ±lmÄ±ÅŸ formatta dÃ¶ndÃ¼rmelisin.

---

## ðŸš« HABER OLMAYAN Ä°Ã‡ERÄ°KLER (ATLANMALI):

1. **REKLAMLAR:**
   - ÃœrÃ¼n/hizmet tanÄ±tÄ±mlarÄ± ("...satÄ±n alÄ±n", "...arayÄ±n", "kampanya", "indirim")
   - Marka/ÅŸirket isimleri (Ã¼rÃ¼n reklamÄ± baÄŸlamÄ±nda)
   - Telefon numaralarÄ±, web siteleri (reklam baÄŸlamÄ±nda)
   - "Sponsorumuz", "DestekÃ§imiz" gibi ifadeler

2. **MÃœZÄ°K PROGRAMLARI:**
   - ÅžarkÄ± sÃ¶zleri
   - ÅžarkÄ±cÄ±/albÃ¼m tanÄ±tÄ±mlarÄ±
   - MÃ¼zik listesi/chart haberleri
   - "Bu hafta en Ã§ok dinlenen" gibi iÃ§erikler

3. **PROGRAM Ä°Ã‡ERÄ°KLERÄ°:**
   - Sunucu sohbetleri (haber dÄ±ÅŸÄ±)
   - Program tanÄ±tÄ±mlarÄ±
   - Dinleyici mesajlarÄ±
   - Jingle'lar, ara mÃ¼zikleri

4. **DÄ°ÄžER:**
   - Hava durumu tahmini (Ã¶nemli hava olayÄ± deÄŸilse)
   - BugÃ¼n tarihte ne oldu
   - Horoskop/astroloji
   - EÄŸlence/magazin (Ã¶nemsiz dedikodu)

---

## âœ… HABER OLAN Ä°Ã‡ERÄ°KLER (ALINMALI):

1. **POLÄ°TÄ°KA:**
   - HÃ¼kÃ¼met kararlarÄ±, yasalar
   - SeÃ§imler, referandumlar
   - Siyasi aÃ§Ä±klamalar (Ã¶nemli)
   - UluslararasÄ± iliÅŸkiler

2. **EKONOMÄ°:**
   - Ekonomik veriler (enflasyon, bÃ¼yÃ¼me, iÅŸsizlik)
   - Borsa, dÃ¶viz, altÄ±n haberleri
   - Åžirket haberleri (Ã¶nemli geliÅŸmeler)
   - Ekonomi politikalarÄ±

3. **SPOR:**
   - MaÃ§ sonuÃ§larÄ± (Ã¶nemli mÃ¼sabakalar)
   - Transfer haberleri
   - Åžampiyonluklar, milli takÄ±m
   - Spor politikalarÄ±

4. **GÃœNCEL OLAYLAR:**
   - Kazalar, yangÄ±nlar, doÄŸal afetler
   - SuÃ§ haberleri (Ã¶nemli)
   - Protestolar, toplumsal olaylar
   - Mahkeme kararlarÄ±

5. **SAÄžLIK & BÄ°LÄ°M:**
   - SalgÄ±nlar, aÅŸÄ±lar
   - Bilimsel keÅŸifler
   - SaÄŸlÄ±k politikalarÄ±

6. **TEKNOLOJÄ°:**
   - Ã–nemli teknoloji geliÅŸmeleri
   - Siber gÃ¼venlik olaylarÄ±
   - Yapay zeka, uzay haberleri

7. **KÃœLTÃœR & EÄžÄ°TÄ°M:**
   - Ã–nemli kÃ¼ltÃ¼rel etkinlikler
   - EÄŸitim politikalarÄ±
   - Ãœniversite geliÅŸmeleri

---

## ðŸ“‹ Ã‡IKTI FORMATI:

Every haber iÃ§in ÅŸu bilgileri Ã§Ä±kar:

```json
{
  "baslik": "KÄ±sa, Ã¶z baÅŸlÄ±k (5-10 kelime)",
  "kategori": "politika|ekonomi|spor|saglik|teknoloji|guncel|diger",
  "ozet": "2-3 cÃ¼mlelik Ã¶zet. Ana olay ve sonucu iÃ§ermeli.",
  "tam_metin": "Haberin transkriptteki tam metni (aynen)",
  "tarih": "Metinde geÃ§iyorsa tarih/saat bilgisi (Ã¶rn: '3 AralÄ±k 2024', '15:30')",
  "kisiler": ["Metinde geÃ§en kiÅŸi isimleri (politikacÄ±, spor cu, bilim insanÄ±, vb)"],
  "kurumlar": ["Bahsedilen kurum/kuruluÅŸlar (bakanlÄ±k, ÅŸirket, parti, vb)"],
  "yerler": ["Bahsedilen ÅŸehir/Ã¼lke/bÃ¶lge isimleri"],
  "ozel_isimler": ["BÃœTÃœN Ã¶zel isimler - kiÅŸi, kurum, yer, marka, Ã¼rÃ¼n, etkinlik, proje adlarÄ± - BÃ¼yÃ¼k harfle baÅŸlayan TÃœM isimler"]
}
```

**Ã–ZEL Ä°SÄ°MLER (ozel_isimler) KURALI:**
- Metinde geÃ§en TÃœM proper noun'larÄ± (Ã¶zel isimleri) Ã§Ä±kar
- KiÅŸi adlarÄ±: "Recep Tayyip ErdoÄŸan", "Lionel Messi"
- Kurum/Åžirket: "TÃ¼rkiye Cumhuriyeti", "Apple", "NATO"
- Yerler: "Ä°stanbul", "Avrupa BirliÄŸi", "BoÄŸaziÃ§i KÃ¶prÃ¼sÃ¼"
- Marka/ÃœrÃ¼n: "iPhone 15", "Tesla Model 3"
- Etkinlik/Proje: "DÃ¼nya KupasÄ±", "Kanal Ä°stanbul"
- YasanÄ±ÅŸlarÄ±na gÃ¶re bÃ¼yÃ¼k harfle yazÄ±lan HER ÅžEY
- Tekrar olabilir, sorun deÄŸil - hepsini listele

---

## âš ï¸ Ã–NEMLÄ° KURALLAR:

1. **NET HABER OLMAYAN HÄ°Ã‡BÄ°R ÅžEY EKLEME**
   - ÅžÃ¼pheli iÃ§erikleri atla
   - "Belki haber olabilir" deme, emin ol

2. **REKLAM TESPÄ°TÄ°:**
   - ÃœrÃ¼n/marka ismi + Ã¶vgÃ¼ = REKLAM
   - Telefon/web adresi = REKLAM
   - "Kampanya", "indirim", "satÄ±n al" = REKLAM

3. **KATEGORÄ° SINIFLANDIRMASI:**
   - Her haberi en uygun kategoriye ata
   - KararsÄ±zsan "guncel" kullan
   - "diger" sadece hiÃ§biri uymuyorsa

4. **Ã–Z VE NET OL:**
   - BaÅŸlÄ±k: KÄ±sa ve aÃ§Ä±klayÄ±cÄ±
   - Ã–zet: Sadece Ã¶nemli bilgiler
   - Tam metin: Transkriptteki ilgili kÄ±smÄ±n tamamÄ±
   - Ã–zel isimler: Eksik bÄ±rakma, hepsini al

5. **BOÅžLUK OLMASIN:**
   - HiÃ§ haber yoksa bile boÅŸ array dÃ¶n: `{"news_items": []}`
   - `null` veya `undefined` dÃ¶ndÃ¼rme

---

## ðŸ“¤ JSON ÅžEMASI:

```json
{
  "news_items": [
    {
      "baslik": "string",
      "kategori": "politika|ekonomi|spor|saglik|teknoloji|guncel|diger",
      "ozet": "string",
      "tam_metin": "string",
      "tarih": "string veya null",
      "kisiler": ["string"] veya null,
      "kurumlar": ["string"] veya null,
      "yerler": ["string"] veya null,
      "ozel_isimler": ["string"] veya null
    }
  ]
}
```

**SADECE GEÃ‡ERLÄ° JSON DÃ–NDÃœR. AÃ‡IKLAMA YAPMA.**
"""

async def extract_news_from_transcript(transcript: str, api_key: str) -> List[NewsItem]:
    """
    Extract news items from transcript using GPT-4o-mini
    
    Args:
        transcript: Full radio transcript
        api_key: OpenAI API key
    
    Returns:
        List of NewsItem objects
    """
    try:
        print(f"[DEBUG] Extracting news from transcript ({len(transcript)} chars)")
        client = openai.OpenAI(api_key=api_key)
        
        system_prompt = create_news_extraction_prompt()
        user_prompt = f"**RADYO TRANSKRÄ°PTÄ°:**\n\n{transcript}\n\n---\n\n**YukarÄ±daki transkriptten SADECE HABER iÃ§eriklerini JSON formatÄ±nda Ã§Ä±kar:**"
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.1,
            max_tokens=16000,
            response_format={"type": "json_object"}
        )
        
        result_text = response.choices[0].message.content
        result_data = json.loads(result_text)
        
        news_items = []
        for item in result_data.get('news_items', []):
            try:
                news_item = NewsItem(**item)
                news_items.append(news_item)
            except Exception as e:
                print(f"[WARNING] Failed to parse news item: {e}")
                continue
        
        print(f"[DEBUG] Extracted {len(news_items)} news items")
        return news_items
        
    except Exception as e:
        print(f"[ERROR] News extraction failed: {e}")
        raise

def cleanup_temp_files(file_paths: List[Path]):
    """Delete temporary files"""
    for path in file_paths:
        try:
            if path.exists():
                path.unlink()
                print(f"[DEBUG] Deleted: {path}")
        except Exception as e:
            print(f"[WARNING] Failed to delete {path}: {e}")

# API Endpoints

@app.post("/api/v1/pipelines/radyo-news-stream")
async def process_radyo_news_stream(
    file: UploadFile = File(...),
    openai_api_key: Optional[str] = Form(None),
):
    """
    Process radio audio file with Server-Sent Events for real-time progress
    """
    # Validate API key
    if not openai_api_key or not openai_api_key.strip():
        raise HTTPException(
            status_code=400,
            detail="OpenAI API Key gerekli."
        )
    
    async def event_generator():
        temp_files = []
        
        try:
            # Step 1: Save uploaded file
            yield f"data: {json.dumps({'type': 'init', 'message': 'Ses dosyasÄ± yÃ¼klendi, iÅŸleme baÅŸlÄ±yor...'})}\n\n"
            
            # Save audio file
            file_ext = Path(file.filename).suffix if file.filename else '.mp3'
            audio_path = TEMP_AUDIO_DIR / f"radio_{id(file)}{file_ext}"
            temp_files.append(audio_path)
            
            with open(audio_path, 'wb') as f:
                content = await file.read()
                f.write(content)
            
            file_size_mb = len(content) / 1024 / 1024
            yield f"data: {json.dumps({'type': 'progress', 'step': 'uploaded', 'message': f'Dosya yÃ¼klendi ({file_size_mb:.1f} MB)'})}\n\n"
            
            # Step 2: Transcribe with Whisper (auto-chunks if needed)
            yield f"data: {json.dumps({'type': 'progress', 'step': 'transcription', 'message': 'Whisper ile transkript alÄ±nÄ±yor... (1-3 dakika sÃ¼rebilir)'})}\n\n"
            
            transcript = await transcribe_audio(audio_path, openai_api_key)
            
            if not transcript or len(transcript) < 100:
                yield f"data: {json.dumps({'type': 'error', 'message': 'Transkript alÄ±namadÄ± veya Ã§ok kÄ±sa. Ses dosyasÄ±nÄ± kontrol edin.'})}\n\n"
                return
            
            yield f"data: {json.dumps({'type': 'progress', 'step': 'transcribed', 'message': f'âœ“ Transkript alÄ±ndÄ± ({len(transcript)} karakter)'})}\n\n"
            
            # Step 3: Extract news with GPT
            yield f"data: {json.dumps({'type': 'progress', 'step': 'analysis', 'message': 'GPT ile haber analizi yapÄ±lÄ±yor... (30-60 saniye)'})}\n\n"
            
            news_items = await extract_news_from_transcript(transcript, openai_api_key)
            
            yield f"data: {json.dumps({'type': 'progress', 'step': 'analyzed', 'message': f'âœ“ Analiz tamamlandÄ±! {len(news_items)} haber bulundu'})}\n\n"
            
            # Calculate statistics
            categories = {}
            for news in news_items:
                categories[news.kategori] = categories.get(news.kategori, 0) + 1
            
            # Step 4: Send complete result
            result = RadioAnalysisResult(
                total_news_count=len(news_items),
                categories=categories,
                news_items=news_items,
                raw_transcript=transcript
            )
            
            yield f"data: {json.dumps({'type': 'complete', 'result': result.dict(), 'message': f'âœ“ TamamlandÄ±! {len(news_items)} haber bulundu'})}\n\n"
            
        except Exception as e:
            yield f"data: {json.dumps({'type': 'error', 'message': f'Hata: {str(e)}'})}\n\n"
        
        finally:
            # Cleanup
            cleanup_temp_files(temp_files)
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "X-Accel-Buffering": "no"
        }
    )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=8008, reload=False)
