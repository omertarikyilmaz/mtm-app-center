FROM nvidia/cuda:12.1.0-devel-ubuntu22.04

WORKDIR /app

# Install python and pip
RUN apt-get update && apt-get install -y python3 python3-pip git && \
    rm -rf /var/lib/apt/lists/*

# Install vLLM nightly
# We use uv for faster and more reliable resolution, and to support --index-strategy
RUN pip3 install uv && \
    uv pip install --system -U vllm --pre --extra-index-url https://wheels.vllm.ai/nightly --extra-index-url https://download.pytorch.org/whl/cu129 --index-strategy unsafe-best-match && \
    uv pip install --system bitsandbytes

# Expose port
EXPOSE 8002

# Command to run vLLM
# Note: Arguments should be passed via docker-compose command or overridden here
CMD ["vllm", "serve", "RedHatAI/Qwen3-8B-FP8-dynamic", "--gpu-memory-utilization", "0.45", "--port", "8002", "--trust-remote-code"]
